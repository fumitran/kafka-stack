"""
Li·ªát k√™ to√†n b·ªô th√¥ng tin chi ti·∫øt c·ªßa consumer trong c√°c topic c√≥ trong cluster ch·ªâ ƒë·ªãnh
(l·∫•y ra th√¥ng tin consumer ƒëang k·∫øt n·ªëi v√†o n√≥) v√† xu·∫•t k·∫øt qu·∫£ v√†o file CSV.
"""

import argparse
import csv
import json
import logging
import os
import re
import sys
import traceback
from datetime import datetime
from typing import Any, Dict, List, Optional

from kafka_ui_client import KafkaUIClient
from config import KafkaUIConfig

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('kafka_export.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


def ensure_export_dir() -> str:
    """ƒê·∫£m b·∫£o t·ªìn t·∫°i th∆∞ m·ª•c export, tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi."""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    export_dir = os.path.join(base_dir, "export")
    os.makedirs(export_dir, exist_ok=True)
    return export_dir


def sanitize_name_for_filename(name: str) -> str:
    """Chu·∫©n h√≥a t√™n ƒë·ªÉ d√πng trong t√™n file: ch·ªâ gi·ªØ [A-Za-z0-9_.-], c√≤n l·∫°i thay b·∫±ng '_'."""
    if not name:
        return "default"
    # Thay m·ªçi chu·ªói k√Ω t·ª± kh√¥ng h·ª£p l·ªá b·∫±ng d·∫•u g·∫°ch d∆∞·ªõi
    safe = re.sub(r"[^A-Za-z0-9_.-]+", "_", name.strip())
    return safe or "default"


def normalize_row(data: Dict[str, Any], fieldnames: List[str]) -> Dict[str, str]:
    """
    Chuy·ªÉn dict sang dict string cho CSV.
    - Gi√° tr·ªã None -> "".
    - List / Dict -> json.dumps.
    """
    row: Dict[str, str] = {}
    for key in fieldnames:
        value = data.get(key)
        if value is None:
            row[key] = ""
        elif isinstance(value, (dict, list)):
            row[key] = json.dumps(value, ensure_ascii=False)
        else:
            row[key] = str(value)
    return row


def get_consumers_for_topic(
    client: KafkaUIClient,
    topic_name: str,
    cluster_name: str
) -> List[Dict[str, Any]]:
    """
    L·∫•y th√¥ng tin consumers ƒëang consume m·ªôt topic c·ª• th·ªÉ.
    
    Args:
        client: KafkaUIClient instance
        topic_name: T√™n topic
        cluster_name: T√™n cluster
        
    Returns:
        List c√°c consumer records cho topic n√†y
    """
    consumers = []
    
    try:
        # L·∫•y danh s√°ch consumer groups ƒëang consume topic n√†y
        consumer_groups = client.get_topic_consumer_groups(topic_name, cluster_name)
        
        if not consumer_groups:
            return consumers
        
        # Duy·ªát qua t·ª´ng consumer group ƒë·ªÉ l·∫•y th√¥ng tin chi ti·∫øt
        for consumer_group in consumer_groups:
            consumer_group_id = consumer_group.get('groupId') or consumer_group.get('id')
            if not consumer_group_id:
                continue
            
            try:
                # L·∫•y th√¥ng tin chi ti·∫øt c·ªßa consumer group
                try:
                    group_details = client.get_consumer_group_details(consumer_group_id, cluster_name)
                except Exception as e:
                    error_msg = str(e)
                    if '404' in error_msg or 'Not Found' in error_msg:
                        logger.debug(f"    Consumer group '{consumer_group_id}' kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng th·ªÉ truy c·∫≠p.")
                        continue
                    else:
                        logger.warning(f"    Kh√¥ng th·ªÉ l·∫•y chi ti·∫øt cho consumer group '{consumer_group_id}': {e}")
                        group_details = consumer_group
                
                # L·∫•y offsets ƒë·ªÉ bi·∫øt chi ti·∫øt partitions
                offsets_data = None
                try:
                    offsets_data = client.get_consumer_group_offsets(consumer_group_id, cluster_name)
                except Exception as e:
                    error_msg = str(e)
                    if '404' in error_msg or 'Not Found' in error_msg:
                        logger.debug(f"    Consumer group '{consumer_group_id}' kh√¥ng c√≥ offsets endpoint.")
                    else:
                        logger.debug(f"    Kh√¥ng th·ªÉ l·∫•y offsets cho consumer group '{consumer_group_id}': {e}")
                    
                    # Th·ª≠ l·∫•y t·ª´ group_details
                    if isinstance(group_details, dict) and 'offsets' in group_details:
                        offsets_data = group_details.get('offsets')
                
                # Parse offsets ƒë·ªÉ l·∫•y th√¥ng tin partitions
                offsets_list = []
                if offsets_data:
                    if isinstance(offsets_data, list):
                        offsets_list = offsets_data
                    elif isinstance(offsets_data, dict):
                        offsets_list = offsets_data.get('offsets', [])
                        if not offsets_list and 'topic' in offsets_data:
                            offsets_list = [offsets_data]
                
                # L·ªçc ch·ªâ l·∫•y offsets c·ªßa topic n√†y
                topic_offsets = []
                for offset_info in offsets_list:
                    if isinstance(offset_info, dict):
                        offset_topic = offset_info.get('topic')
                        if offset_topic == topic_name:
                            topic_offsets.append(offset_info)
                
                # L·∫•y th√¥ng tin members t·ª´ partitions trong group_details
                # N·∫øu kh√¥ng c√≥ offsets t·ª´ API, th·ª≠ l·∫•y t·ª´ partitions trong group_details
                if not topic_offsets and isinstance(group_details, dict) and 'partitions' in group_details:
                    partitions = group_details.get('partitions', [])
                    for partition_info in partitions:
                        if isinstance(partition_info, dict):
                            partition_topic = partition_info.get('topic')
                            if partition_topic == topic_name:
                                topic_offsets.append(partition_info)
                
                # Extract th√¥ng tin members t·ª´ partitions (ch·ªâ l·∫•y c·ªßa topic n√†y)
                # T·∫°o set ƒë·ªÉ check tr√πng d·ª±a tr√™n (topic_name, consumer_group_id, consumer_id, host)
                seen_combinations = set()
                
                # L·∫•y th√¥ng tin t·ª´ partitions c·ªßa topic n√†y
                if isinstance(group_details, dict) and 'partitions' in group_details:
                    partitions = group_details.get('partitions', [])
                    for partition_info in partitions:
                        if isinstance(partition_info, dict):
                            partition_topic = partition_info.get('topic')
                            if partition_topic == topic_name:
                                consumer_id = partition_info.get('consumerId')
                                host = partition_info.get('host')
                                
                                # T·∫°o key ƒë·ªÉ check tr√πng
                                if consumer_id:
                                    combination_key = (topic_name, consumer_group_id, consumer_id, host or '')
                                    
                                    # Ch·ªâ th√™m n·∫øu ch∆∞a c√≥
                                    if combination_key not in seen_combinations:
                                        seen_combinations.add(combination_key)
                                        
                                        # T·∫°o record v·ªõi 4 tr∆∞·ªùng c·∫ßn thi·∫øt
                                        consumer_record = {
                                            'topicName': topic_name,
                                            'consumerGroupId': consumer_group_id,
                                            'consumerClientId': consumer_id or '',
                                            'consumerClientHost': host or '',
                                        }
                                        consumers.append(consumer_record)
                
                # N·∫øu kh√¥ng c√≥ partitions ho·∫∑c kh√¥ng t√¨m th·∫•y consumer info trong partitions
                # Th·ª≠ l·∫•y t·ª´ topic_offsets
                if not consumers and topic_offsets:
                    for offset_info in topic_offsets:
                        if isinstance(offset_info, dict):
                            consumer_id = offset_info.get('consumerId')
                            host = offset_info.get('host')
                            
                            if consumer_id:
                                combination_key = (topic_name, consumer_group_id, consumer_id, host or '')
                                
                                if combination_key not in seen_combinations:
                                    seen_combinations.add(combination_key)
                                    
                                    consumer_record = {
                                        'topicName': topic_name,
                                        'consumerGroupId': consumer_group_id,
                                        'consumerClientId': consumer_id or '',
                                        'consumerClientHost': host or '',
                                    }
                                    consumers.append(consumer_record)
                
                # N·∫øu v·∫´n kh√¥ng c√≥ consumer info, t·∫°o record v·ªõi consumer info r·ªóng
                if not consumers:
                    combination_key = (topic_name, consumer_group_id, '', '')
                    if combination_key not in seen_combinations:
                        seen_combinations.add(combination_key)
                        consumer_record = {
                            'topicName': topic_name,
                            'consumerGroupId': consumer_group_id,
                            'consumerClientId': '',
                            'consumerClientHost': '',
                        }
                        consumers.append(consumer_record)
                
            except Exception as e:
                logger.warning(f"  Kh√¥ng th·ªÉ x·ª≠ l√Ω consumer group '{consumer_group_id}' cho topic '{topic_name}': {e}")
                logger.debug(f"  Traceback: {traceback.format_exc()}")
                continue
                
    except Exception as e:
        error_msg = str(e)
        if '404' in error_msg or 'Not Found' in error_msg:
            # Topic kh√¥ng c√≥ consumer groups - ƒë√¢y l√† tr∆∞·ªùng h·ª£p b√¨nh th∆∞·ªùng
            logger.debug(f"  Topic '{topic_name}' kh√¥ng c√≥ consumer groups.")
        else:
            logger.warning(f"  L·ªói khi l·∫•y consumer groups cho topic '{topic_name}': {e}")
    
    return consumers


def get_all_consumers_for_cluster(
    client: KafkaUIClient,
    topics: List[Dict[str, Any]],
    cluster_name: str
) -> Dict[str, List[Dict[str, Any]]]:
    """
    L·∫•y th√¥ng tin t·∫•t c·∫£ consumers trong cluster, nh√≥m theo topic.
    
    Args:
        client: KafkaUIClient instance
        topics: Danh s√°ch topics
        cluster_name: T√™n cluster
        
    Returns:
        Dict v·ªõi key l√† topic_name, value l√† list c√°c consumer records
    """
    topic_consumers_map: Dict[str, List[Dict[str, Any]]] = {}
    
    logger.info(f"  ƒêang l·∫•y consumer groups cho t·ª´ng topic...")
    
    for idx, topic in enumerate(topics, 1):
        topic_name = topic.get('name')
        if not topic_name:
            continue
        
        try:
            logger.info(f"  [{idx}/{len(topics)}] ƒêang l·∫•y consumer groups cho topic '{topic_name}'...")
            consumers = get_consumers_for_topic(client, topic_name, cluster_name)
            
            if consumers:
                topic_consumers_map[topic_name] = consumers
                logger.info(f"    ‚Üí T√¨m th·∫•y {len(consumers)} consumer record(s)")
            else:
                logger.debug(f"    ‚Üí Kh√¥ng c√≥ consumer groups")
                
        except Exception as e:
            logger.warning(f"  L·ªói khi l·∫•y consumer groups cho topic '{topic_name}': {e}")
            continue
    
    return topic_consumers_map


def remove_duplicate_consumers(consumers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Lo·∫°i b·ªè c√°c consumer records tr√πng l·∫∑p d·ª±a tr√™n 4 tr∆∞·ªùng:
    topicName, consumerGroupId, consumerClientId, consumerClientHost
    
    Args:
        consumers: Danh s√°ch consumer records
        
    Returns:
        Danh s√°ch consumer records ƒë√£ lo·∫°i b·ªè tr√πng l·∫∑p
    """
    seen = set()
    unique_consumers = []
    
    for consumer in consumers:
        topic_name = consumer.get('topicName', '')
        consumer_group_id = consumer.get('consumerGroupId', '')
        consumer_client_id = consumer.get('consumerClientId', '')
        consumer_client_host = consumer.get('consumerClientHost', '')
        
        # T·∫°o key ƒë·ªÉ check tr√πng
        key = (topic_name, consumer_group_id, consumer_client_id, consumer_client_host)
        
        if key not in seen:
            seen.add(key)
            unique_consumers.append(consumer)
    
    return unique_consumers


def export_consumers_to_csv(consumers: List[Dict[str, Any]], file_path: str) -> None:
    """Ghi danh s√°ch consumers ra file CSV v·ªõi ch·ªâ 4 c·ªôt."""
    if not consumers:
        logger.warning("Kh√¥ng c√≥ consumer n√†o ƒë·ªÉ export.")
        return

    # Lo·∫°i b·ªè tr√πng l·∫∑p
    unique_consumers = remove_duplicate_consumers(consumers)
    logger.info(f"  ƒê√£ lo·∫°i b·ªè {len(consumers) - len(unique_consumers)} record(s) tr√πng l·∫∑p.")

    # Ch·ªâ export 4 c·ªôt theo y√™u c·∫ßu
    fieldnames = [
        "topicName",
        "consumerGroupId",
        "consumerClientId",
        "consumerClientHost",
    ]

    with open(file_path, mode="w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for c in unique_consumers:
            # Ch·ªâ l·∫•y 4 tr∆∞·ªùng c·∫ßn thi·∫øt
            row = {
                'topicName': c.get('topicName', ''),
                'consumerGroupId': c.get('consumerGroupId', ''),
                'consumerClientId': c.get('consumerClientId', ''),
                'consumerClientHost': c.get('consumerClientHost', ''),
            }
            writer.writerow(normalize_row(row, fieldnames))

    logger.info(f"‚úÖ ƒê√£ export {len(unique_consumers)} consumer records (unique) ra file: {file_path}")


def export_consumers_for_cluster(
    client: KafkaUIClient,
    cluster_name: str,
    export_dir: str
) -> None:
    """
    Export t·∫•t c·∫£ th√¥ng tin consumers c·ªßa c√°c topics trong cluster ra CSV.
    
    Args:
        client: KafkaUIClient instance
        cluster_name: T√™n cluster c·∫ßn export
        export_dir: Th∆∞ m·ª•c export
    """
    try:
        logger.info(f"üìã ƒêang l·∫•y T·∫§T C·∫¢ topics t·ª´ cluster '{cluster_name}'...")
        topics = client.get_topics(cluster_name)
        
        if not topics:
            logger.warning(f"‚ö†Ô∏è  Cluster '{cluster_name}' kh√¥ng c√≥ topic n√†o.")
            return
        
        logger.info(f"‚úÖ L·∫•y ƒë∆∞·ª£c {len(topics)} topics t·ª´ cluster '{cluster_name}'.")
        
        # L·∫•y th√¥ng tin consumers cho t·∫•t c·∫£ topics
        logger.info("üìä ƒêang l·∫•y th√¥ng tin consumers cho t·ª´ng topic...")
        topic_consumers_map = get_all_consumers_for_cluster(client, topics, cluster_name)
        
        # T·∫°o danh s√°ch t·∫•t c·∫£ consumers
        all_consumers = []
        topics_with_consumers = 0
        
        for topic in topics:
            topic_name = topic.get('name')
            if not topic_name:
                continue
            
            consumers = topic_consumers_map.get(topic_name, [])
            if consumers:
                all_consumers.extend(consumers)
                topics_with_consumers += 1
                logger.info(f"  Topic '{topic_name}': {len(consumers)} consumer record(s)")
            else:
                # N·∫øu topic kh√¥ng c√≥ consumer, v·∫´n t·∫°o m·ªôt record v·ªõi consumer info r·ªóng
                empty_consumer_record = {
                    'topicName': topic_name,
                    'consumerGroupId': '',
                    'consumerClientId': '',
                    'consumerClientHost': '',
                }
                all_consumers.append(empty_consumer_record)
                logger.debug(f"  Topic '{topic_name}': Kh√¥ng c√≥ consumer (t·∫°o record r·ªóng)")
        
        logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω {len(topics)} topics (c√≥ consumers: {topics_with_consumers}, kh√¥ng c√≥ consumers: {len(topics) - topics_with_consumers}).")
        
        # Lu√¥n export file CSV, k·ªÉ c·∫£ khi kh√¥ng c√≥ consumer
        if not all_consumers:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng c√≥ topic n√†o ƒë·ªÉ export.")
            return
        
        # T·∫°o t√™n file v·ªõi timestamp
        safe_cluster_name = sanitize_name_for_filename(cluster_name)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(export_dir, f"topic_{safe_cluster_name}_consumer_{timestamp}.csv")
        
        export_consumers_to_csv(all_consumers, output_file)
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi export consumers t·ª´ cluster '{cluster_name}'", exc_info=True)
        logger.error(f"Traceback:\n{traceback.format_exc()}")


def main():
    """Main: g·ªçi Kafka UI v√† export consumer information ra CSV."""
    parser = argparse.ArgumentParser(
        description="Export th√¥ng tin consumers t·ª´ c√°c topics trong cluster(s) ra CSV"
    )
    parser.add_argument(
        "--cluster",
        type=str,
        default=None,
        help="T√™n cluster c·∫ßn export (n·∫øu kh√¥ng ch·ªâ ƒë·ªãnh s·∫Ω d√πng cluster_name t·ª´ config.cfg)"
    )
    parser.add_argument(
        "--all-clusters",
        action="store_true",
        help="Export consumers t·ª´ T·∫§T C·∫¢ clusters"
    )
    
    args = parser.parse_args()
    
    # C·∫•u h√¨nh ƒë·ªçc t·ª´ config.cfg / bi·∫øn m√¥i tr∆∞·ªùng / tham s·ªë trong KafkaUIConfig
    config = KafkaUIConfig()
    client = KafkaUIClient(config)
    
    export_dir = ensure_export_dir()
    
    try:
        if args.all_clusters:
            # Export t·∫•t c·∫£ clusters
            logger.info("üîÑ ƒêang l·∫•y danh s√°ch T·∫§T C·∫¢ clusters...")
            clusters = client.get_clusters()
            logger.info(f"T√¨m th·∫•y {len(clusters)} clusters.")
            
            for cluster in clusters:
                cluster_name = cluster.get('name')
                if cluster_name:
                    export_consumers_for_cluster(client, cluster_name, export_dir)
        else:
            # Export cluster ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
            cluster_name = args.cluster or config.cluster_name
            if not cluster_name:
                logger.error("‚ùå Kh√¥ng c√≥ cluster n√†o ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh!")
                logger.info("üí° S·ª≠ d·ª•ng: --cluster <t√™n_cluster> ho·∫∑c --all-clusters")
                return
            
            export_consumers_for_cluster(client, cluster_name, export_dir)
            
    except Exception as e:
        logger.error(f"‚ùå L·ªói ch√≠nh: {e}", exc_info=True)
        logger.error(f"Traceback ƒë·∫ßy ƒë·ªß:\n{traceback.format_exc()}")
        logger.info("\nüí° G·ª£i √Ω:")
        logger.info("  1. ƒê·∫£m b·∫£o Kafka UI ƒëang ch·∫°y v√† c·∫•u h√¨nh ƒë√∫ng trong config.cfg")
        logger.info("  2. Ki·ªÉm tra SESSION cookie ho·∫∑c username/password c√≥ ƒë√∫ng kh√¥ng")
        logger.info("  3. Ki·ªÉm tra t√™n cluster c√≥ ƒë√∫ng kh√¥ng")
        logger.info("  4. Th·ª≠: python main_list_topic_consumer.py --all-clusters")
        sys.exit(1)


if __name__ == "__main__":
    main()

