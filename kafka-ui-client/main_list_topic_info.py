"""
Li·ªát k√™ to√†n b·ªô th√¥ng tin c√°c topic trong m·ªôt cluster v√† ghi ra file CSV.
"""

import csv
import json
import os
import re
from typing import Any, Dict, List

from kafka_ui_client import KafkaUIClient
from config import KafkaUIConfig


def ensure_export_dir() -> str:
    """ƒê·∫£m b·∫£o t·ªìn t·∫°i th∆∞ m·ª•c export, tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi."""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    export_dir = os.path.join(base_dir, "export")
    os.makedirs(export_dir, exist_ok=True)
    return export_dir


def sanitize_name_for_filename(name: str) -> str:
    """Chu·∫©n h√≥a t√™n ƒë·ªÉ d√πng trong t√™n file: ch·ªâ gi·ªØ [A-Za-z0-9_.-], c√≤n l·∫°i thay b·∫±ng '_'."""
    if not name:
        return "default"
    # Thay m·ªçi chu·ªói k√Ω t·ª± kh√¥ng h·ª£p l·ªá b·∫±ng d·∫•u g·∫°ch d∆∞·ªõi
    safe = re.sub(r"[^A-Za-z0-9_.-]+", "_", name.strip())
    return safe or "default"


def normalize_row(data: Dict[str, Any], fieldnames: List[str]) -> Dict[str, str]:
    """
    Chuy·ªÉn dict sang dict string cho CSV.
    - Gi√° tr·ªã None -> "".
    - List / Dict -> json.dumps.
    """
    row: Dict[str, str] = {}
    for key in fieldnames:
        value = data.get(key)
        if value is None:
            row[key] = ""
        elif isinstance(value, (dict, list)):
            row[key] = json.dumps(value, ensure_ascii=False)
        else:
            row[key] = str(value)
    return row


def export_topics_to_csv(topics: List[Dict[str, Any]], file_path: str) -> None:
    """Ghi danh s√°ch topics ra file CSV."""
    if not topics:
        print("Kh√¥ng c√≥ topic n√†o ƒë·ªÉ export.")
        return

    # L·∫•y t·∫≠p h·ª£p t·∫•t c·∫£ key xu·∫•t hi·ªán trong c√°c topic (ƒë·∫£m b·∫£o ƒë·ªß c·ªôt)
    fieldnames_set = set()
    for t in topics:
        fieldnames_set.update(t.keys())

    # S·∫Øp x·∫øp c·ªôt, ∆∞u ti√™n m·ªôt s·ªë c·ªôt hay d√πng l√™n ƒë·∫ßu
    preferred_order = [
        "name",
        "internal",
        "partitionCount",
        "replicationFactor",
        "replicas",
        "inSyncReplicas",
        "segmentSize",
        "segmentCount",
        "bytesInPerSec",
        "bytesOutPerSec",
        "underReplicatedPartitions",
        "cleanUpPolicy",
    ]
    remaining = [f for f in sorted(fieldnames_set) if f not in preferred_order]
    fieldnames = [f for f in preferred_order if f in fieldnames_set] + remaining

    with open(file_path, mode="w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for t in topics:
            writer.writerow(normalize_row(t, fieldnames))

    print(f"‚úÖ ƒê√£ export {len(topics)} topics ra file: {file_path}")


def main():
    """Main: g·ªçi Kafka UI v√† export topics ra CSV."""
    # C·∫•u h√¨nh ƒë·ªçc t·ª´ config.cfg / bi·∫øn m√¥i tr∆∞·ªùng / tham s·ªë trong KafkaUIConfig
    config = KafkaUIConfig()
    client = KafkaUIClient(config)

    cluster_name = config.cluster_name

    try:
        print(f"ƒêang l·∫•y danh s√°ch topics t·ª´ cluster '{cluster_name}'...")
        topics = client.get_topics(cluster_name)
        print(f"L·∫•y ƒë∆∞·ª£c {len(topics)} topics.")

        export_dir = ensure_export_dir()
        safe_cluster_name = sanitize_name_for_filename(cluster_name)
        output_file = os.path.join(export_dir, f"topic_{safe_cluster_name}_info.csv")
        export_topics_to_csv(topics, output_file)
    except Exception as e:
        print(f"\n‚ùå L·ªói khi export topics: {e}")
        print("\nüí° G·ª£i √Ω:")
        print("  1. ƒê·∫£m b·∫£o Kafka UI ƒëang ch·∫°y v√† c·∫•u h√¨nh cluster ƒë√∫ng trong config.cfg")
        print("  2. Ki·ªÉm tra SESSION cookie ho·∫∑c username/password c√≥ ƒë√∫ng kh√¥ng")
        print(f"  3. Th·ª≠ g·ªçi tr·ª±c ti·∫øp API: curl http://localhost:8080/api/clusters/{cluster_name}/topics")


if __name__ == "__main__":
    main()


