"""
Liá»‡t kÃª toÃ n bá»™ thÃ´ng tin cÃ¡c topic trong má»™t cluster vÃ  ghi ra file CSV.
CÃ³ thá»ƒ chá»‰ Ä‘á»‹nh cluster tá»« command line hoáº·c export táº¥t cáº£ clusters.
"""

import argparse
import csv
import json
import logging
import os
import re
import sys
import traceback
from datetime import datetime
from typing import Any, Dict, List, Optional

from kafka_ui_client import KafkaUIClient
from config import KafkaUIConfig

# Cáº¥u hÃ¬nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('kafka_export.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


def ensure_export_dir() -> str:
    """Äáº£m báº£o tá»“n táº¡i thÆ° má»¥c export, tráº£ vá» Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i."""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    export_dir = os.path.join(base_dir, "export")
    os.makedirs(export_dir, exist_ok=True)
    return export_dir


def sanitize_name_for_filename(name: str) -> str:
    """Chuáº©n hÃ³a tÃªn Ä‘á»ƒ dÃ¹ng trong tÃªn file: chá»‰ giá»¯ [A-Za-z0-9_.-], cÃ²n láº¡i thay báº±ng '_'."""
    if not name:
        return "default"
    # Thay má»i chuá»—i kÃ½ tá»± khÃ´ng há»£p lá»‡ báº±ng dáº¥u gáº¡ch dÆ°á»›i
    safe = re.sub(r"[^A-Za-z0-9_.-]+", "_", name.strip())
    return safe or "default"


def normalize_row(data: Dict[str, Any], fieldnames: List[str]) -> Dict[str, str]:
    """
    Chuyá»ƒn dict sang dict string cho CSV.
    - GiÃ¡ trá»‹ None -> "".
    - List / Dict -> json.dumps.
    """
    row: Dict[str, str] = {}
    for key in fieldnames:
        value = data.get(key)
        if value is None:
            row[key] = ""
        elif isinstance(value, (dict, list)):
            row[key] = json.dumps(value, ensure_ascii=False)
        else:
            row[key] = str(value)
    return row


def get_topic_message_count_from_data(topic_data: Dict[str, Any]) -> Optional[int]:
    """
    Láº¥y tá»•ng sá»‘ messages cá»§a má»™t topic tá»« dá»¯ liá»‡u topic Ä‘Ã£ cÃ³ (khÃ´ng cáº§n gá»i API thÃªm).
    
    CÃ¡ch láº¥y:
    1. Response tá»« get_topics() Ä‘Ã£ cÃ³ field 'partitions' - danh sÃ¡ch cÃ¡c partitions
    2. Má»—i partition cÃ³ 'offsetMax' (high watermark) vÃ  'offsetMin' (low watermark)
    3. Sá»‘ messages trong má»—i partition = offsetMax - offsetMin
    4. Tá»•ng sá»‘ messages trong topic = tá»•ng (offsetMax - offsetMin) cá»§a táº¥t cáº£ partitions
    
    Giáº£i thÃ­ch:
    - offsetMax: offset cao nháº¥t (high watermark) - offset cá»§a message cuá»‘i cÃ¹ng + 1
    - offsetMin: offset tháº¥p nháº¥t (low watermark) - offset cá»§a message Ä‘áº§u tiÃªn cÃ²n tá»“n táº¡i
    - offsetMax - offsetMin = sá»‘ lÆ°á»£ng messages thá»±c táº¿ trong partition (sau khi trá»« cÃ¡c message Ä‘Ã£ bá»‹ xÃ³a/compact)
    
    Args:
        topic_data: Dict chá»©a thÃ´ng tin topic tá»« get_topics() response
        
    Returns:
        Tá»•ng sá»‘ messages hoáº·c None náº¿u khÃ´ng láº¥y Ä‘Æ°á»£c
    """
    try:
        partitions = topic_data.get('partitions', [])
        
        if not partitions:
            return None
        
        total_messages = 0
        for partition in partitions:
            offset_max = partition.get('offsetMax')
            offset_min = partition.get('offsetMin')
            
            # Kiá»ƒm tra cáº£ offsetMax vÃ  offsetMin Ä‘á»u cÃ³ giÃ¡ trá»‹ há»£p lá»‡
            if (offset_max is not None and isinstance(offset_max, (int, float)) and
                offset_min is not None and isinstance(offset_min, (int, float))):
                # Sá»‘ messages trong partition = offsetMax - offsetMin
                partition_messages = int(offset_max) - int(offset_min)
                if partition_messages > 0:
                    total_messages += partition_messages
        
        return total_messages if total_messages > 0 else None
    except Exception as e:
        topic_name = topic_data.get('name', 'unknown')
        logger.warning(f"KhÃ´ng thá»ƒ láº¥y sá»‘ lÆ°á»£ng messages cho topic '{topic_name}': {e}")
        return None


def export_topics_to_csv(topics: List[Dict[str, Any]], file_path: str) -> None:
    """Ghi danh sÃ¡ch topics ra file CSV."""
    if not topics:
        logger.warning("KhÃ´ng cÃ³ topic nÃ o Ä‘á»ƒ export.")
        return

    # Loáº¡i bá» trÆ°á»ng 'partitions' khá»i dá»¯ liá»‡u trÆ°á»›c khi export
    topics_clean = []
    for t in topics:
        topic_copy = t.copy()
        topic_copy.pop('partitions', None)  # Bá» trÆ°á»ng partitions náº¿u cÃ³
        topics_clean.append(topic_copy)

    # Láº¥y táº­p há»£p táº¥t cáº£ key xuáº¥t hiá»‡n trong cÃ¡c topic (Ä‘áº£m báº£o Ä‘á»§ cá»™t)
    fieldnames_set = set()
    for t in topics_clean:
        fieldnames_set.update(t.keys())

    # Sáº¯p xáº¿p cá»™t, Æ°u tiÃªn má»™t sá»‘ cá»™t hay dÃ¹ng lÃªn Ä‘áº§u
    preferred_order = [
        "name",
        "totalMessages",  # ThÃªm cá»™t sá»‘ lÆ°á»£ng messages
        "internal",
        "partitionCount",
        "replicationFactor",
        "replicas",
        "inSyncReplicas",
        "segmentSize",
        "segmentCount",
        "bytesInPerSec",
        "bytesOutPerSec",
        "underReplicatedPartitions",
        "cleanUpPolicy",
    ]
    remaining = [f for f in sorted(fieldnames_set) if f not in preferred_order]
    fieldnames = [f for f in preferred_order if f in fieldnames_set] + remaining

    with open(file_path, mode="w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for t in topics_clean:
            writer.writerow(normalize_row(t, fieldnames))

    logger.info(f"âœ… ÄÃ£ export {len(topics)} topics ra file: {file_path}")


def export_topics_for_cluster(
    client: KafkaUIClient,
    cluster_name: str,
    export_dir: str
) -> None:
    """
    Export táº¥t cáº£ topics cá»§a má»™t cluster ra CSV, bao gá»“m sá»‘ lÆ°á»£ng messages.
    
    Args:
        client: KafkaUIClient instance
        cluster_name: TÃªn cluster cáº§n export
        export_dir: ThÆ° má»¥c export
    """
    try:
        logger.info(f"ğŸ“‹ Äang láº¥y Táº¤T Cáº¢ topics tá»« cluster '{cluster_name}'...")
        topics = client.get_topics(cluster_name)
        
        if not topics:
            logger.warning(f"âš ï¸  Cluster '{cluster_name}' khÃ´ng cÃ³ topic nÃ o.")
            return
        
        logger.info(f"âœ… Láº¥y Ä‘Æ°á»£c {len(topics)} topics tá»« cluster '{cluster_name}'.")
        
        # ThÃªm thÃ´ng tin sá»‘ lÆ°á»£ng messages vÃ o má»—i topic (tá»« dá»¯ liá»‡u Ä‘Ã£ cÃ³, khÃ´ng cáº§n gá»i API thÃªm)
        logger.info("ğŸ“Š Äang tÃ­nh sá»‘ lÆ°á»£ng messages cho tá»«ng topic tá»« dá»¯ liá»‡u Ä‘Ã£ cÃ³...")
        for i, topic in enumerate(topics, 1):
            topic_name = topic.get('name')
            if topic_name:
                message_count = get_topic_message_count_from_data(topic)
                if message_count is not None:
                    topic['totalMessages'] = message_count
                    logger.info(f"  [{i}/{len(topics)}] Topic '{topic_name}': {message_count:,} messages")
                else:
                    topic['totalMessages'] = None
                    logger.warning(f"  [{i}/{len(topics)}] Topic '{topic_name}': KhÃ´ng thá»ƒ tÃ­nh sá»‘ lÆ°á»£ng messages")
        
        safe_cluster_name = sanitize_name_for_filename(cluster_name)
        # ThÃªm timestamp vÃ o tÃªn file: format YYYYMMDD_HHMMSS
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(export_dir, f"topic_{safe_cluster_name}_info_{timestamp}.csv")
        export_topics_to_csv(topics, output_file)
        
    except Exception as e:
        logger.error(f"âŒ Lá»—i khi export topics tá»« cluster '{cluster_name}'", exc_info=True)
        logger.error(f"Traceback:\n{traceback.format_exc()}")


def main():
    """Main: gá»i Kafka UI vÃ  export topics ra CSV."""
    parser = argparse.ArgumentParser(
        description="Export táº¥t cáº£ topics tá»« cluster(s) ra CSV"
    )
    parser.add_argument(
        "--cluster",
        type=str,
        default=None,
        help="TÃªn cluster cáº§n export (náº¿u khÃ´ng chá»‰ Ä‘á»‹nh sáº½ dÃ¹ng cluster_name tá»« config.cfg)"
    )
    parser.add_argument(
        "--all-clusters",
        action="store_true",
        help="Export topics tá»« Táº¤T Cáº¢ clusters"
    )
    
    args = parser.parse_args()
    
    # Cáº¥u hÃ¬nh Ä‘á»c tá»« config.cfg / biáº¿n mÃ´i trÆ°á»ng / tham sá»‘ trong KafkaUIConfig
    config = KafkaUIConfig()
    client = KafkaUIClient(config)
    
    export_dir = ensure_export_dir()
    
    try:
        if args.all_clusters:
            # Export táº¥t cáº£ clusters
            logger.info("ğŸ”„ Äang láº¥y danh sÃ¡ch Táº¤T Cáº¢ clusters...")
            clusters = client.get_clusters()
            logger.info(f"TÃ¬m tháº¥y {len(clusters)} clusters.")
            
            for cluster in clusters:
                cluster_name = cluster.get('name')
                if cluster_name:
                    export_topics_for_cluster(client, cluster_name, export_dir)
        else:
            # Export cluster Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh
            cluster_name = args.cluster or config.cluster_name
            if not cluster_name:
                logger.error("âŒ KhÃ´ng cÃ³ cluster nÃ o Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh!")
                logger.info("ğŸ’¡ Sá»­ dá»¥ng: --cluster <tÃªn_cluster> hoáº·c --all-clusters")
                return
            
            export_topics_for_cluster(client, cluster_name, export_dir)
            
    except Exception as e:
        logger.error(f"âŒ Lá»—i chÃ­nh: {e}", exc_info=True)
        logger.error(f"Traceback Ä‘áº§y Ä‘á»§:\n{traceback.format_exc()}")
        logger.info("\nğŸ’¡ Gá»£i Ã½:")
        logger.info("  1. Äáº£m báº£o Kafka UI Ä‘ang cháº¡y vÃ  cáº¥u hÃ¬nh Ä‘Ãºng trong config.cfg")
        logger.info("  2. Kiá»ƒm tra SESSION cookie hoáº·c username/password cÃ³ Ä‘Ãºng khÃ´ng")
        logger.info("  3. Kiá»ƒm tra tÃªn cluster cÃ³ Ä‘Ãºng khÃ´ng")
        logger.info("  4. Thá»­: python main_list_topic_info.py --all-clusters")
        sys.exit(1)


if __name__ == "__main__":
    main()


