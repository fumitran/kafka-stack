"""
R√† so√°t to√†n b·ªô c√°c topic c√≥ trong cluster ch·ªâ ƒë·ªãnh v√† l·∫•y ra 20 th√¥ng tin b·∫£n tin m·ªõi nh·∫•t 
trong topic ƒë√≥, l∆∞u v√†o folder export-msgs_<timestamp>. M·ªói topic t·∫°o th√†nh 1 file JSON array v·ªõi 
t√™n file: <Cluster_name>_<Topic_name>.json
"""

import argparse
import json
import logging
import os
import re
import sys
import traceback
from datetime import datetime
from typing import Any, Dict, List, Optional

import sys
import os
# Add src directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from utils.kafka_ui_client import KafkaUIClient
from config import KafkaUIConfig

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'kafka_export.log'), encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


def ensure_export_msgs_dir() -> str:
    """
    ƒê·∫£m b·∫£o t·ªìn t·∫°i th∆∞ m·ª•c export-msgs_<timestamp> ·ªü root project, tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi.
    - M·ªói l·∫ßn ch·∫°y s·∫Ω t·∫°o m·ªôt folder m·ªõi theo timestamp: YYYYMMDD_HHMMSS
    """
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    export_dir = os.path.join(base_dir, f"export-msgs_{timestamp}")
    os.makedirs(export_dir, exist_ok=True)
    return export_dir


def sanitize_name_for_filename(name: str) -> str:
    """Chu·∫©n h√≥a t√™n ƒë·ªÉ d√πng trong t√™n file: ch·ªâ gi·ªØ [A-Za-z0-9_.-], c√≤n l·∫°i thay b·∫±ng '_'."""
    if not name:
        return "default"
    # Thay m·ªçi chu·ªói k√Ω t·ª± kh√¥ng h·ª£p l·ªá b·∫±ng d·∫•u g·∫°ch d∆∞·ªõi
    safe = re.sub(r"[^A-Za-z0-9_.-]+", "_", name.strip())
    return safe or "default"


def get_latest_messages_from_topic(
    client: KafkaUIClient,
    topic_name: str,
    cluster_name: str,
    limit: int = 20
) -> List[Dict[str, Any]]:
    """
    L·∫•y c√°c message m·ªõi nh·∫•t t·ª´ m·ªôt topic.
    
    Args:
        client: KafkaUIClient instance
        topic_name: T√™n topic
        cluster_name: T√™n cluster
        limit: S·ªë l∆∞·ª£ng messages t·ªëi ƒëa (m·∫∑c ƒë·ªãnh 20)
        
    Returns:
        List c√°c messages (m·ªõi nh·∫•t)
    """
    try:
        # L·∫•y messages t·ª´ topic (kh√¥ng d√πng seekType ƒë·ªÉ tr√°nh l·ªói 400)
        logger.info(f"    üîÑ G·ªçi API l·∫•y {limit} message(s) t·ª´ topic '{topic_name}'...")
        messages = client.get_topic_messages(
            topic_name=topic_name,
            cluster_name=cluster_name,
            limit=limit
        )
        
        if messages:
            logger.info(f"    ‚úÖ Nh·∫≠n ƒë∆∞·ª£c {len(messages)} message(s) t·ª´ topic '{topic_name}'")
        else:
            logger.warning(f"    ‚ö†Ô∏è  Kh√¥ng nh·∫≠n ƒë∆∞·ª£c message n√†o t·ª´ topic '{topic_name}'")
        
        return messages if messages else []
        
    except Exception as e:
        logger.error(f"    ‚ùå L·ªói khi l·∫•y messages t·ª´ topic '{topic_name}': {e}",exc_info=True)
        logger.debug(f"    Traceback: {traceback.format_exc()}")
        return []


def export_messages_to_json(
    messages: List[Dict[str, Any]],
    file_path: str
) -> None:
    """
    Ghi danh s√°ch messages ra file JSON array, ch·ªâ gi·ªØ l·∫°i c√°c tr∆∞·ªùng quan tr·ªçng:
    - key
    - headers
    - content (ho·∫∑c value n·∫øu kh√¥ng c√≥ content)
    
    Args:
        messages: Danh s√°ch messages g·ªëc t·ª´ Kafka UI
        file_path: ƒê∆∞·ªùng d·∫´n file output
    """
    try:
        simplified_messages: List[Dict[str, Any]] = []
        
        for msg in messages:
            if not isinstance(msg, dict):
                continue
            
            key = msg.get("key")
            headers = msg.get("headers")
            content = msg.get("content", msg.get("value"))

            # B·ªè qua record ho√†n to√†n r·ªóng (kh√¥ng c√≥ key, headers, content)
            if key is None and (not headers) and content is None:
                continue

            simplified_messages.append(
                {
                    "key": key,
                    "headers": headers or {},
                    "content": content,
                }
            )

        with open(file_path, mode="w", encoding="utf-8") as f:
            json.dump(simplified_messages, f, ensure_ascii=False, indent=2)
        logger.info(
            f"    ‚úÖ ƒê√£ l∆∞u {len(simplified_messages)} message(s) (ƒë√£ r√∫t g·ªçn) v√†o: {os.path.basename(file_path)}"
        )
    except Exception as e:
        logger.error(f"    ‚ùå L·ªói khi ghi file '{file_path}': {e}")


def extract_messages_for_cluster(
    client: KafkaUIClient,
    cluster_name: str,
    export_dir: str,
    message_limit: int = 20
) -> None:
    """
    R√† so√°t t·∫•t c·∫£ topics trong cluster v√† l·∫•y 20 message m·ªõi nh·∫•t t·ª´ m·ªói topic.
    
    Args:
        client: KafkaUIClient instance
        cluster_name: T√™n cluster c·∫ßn x·ª≠ l√Ω
        export_dir: Th∆∞ m·ª•c export
        message_limit: S·ªë l∆∞·ª£ng messages t·ªëi ƒëa cho m·ªói topic (m·∫∑c ƒë·ªãnh 20)
    """
    try:
        logger.info(f"üìã ƒêang l·∫•y T·∫§T C·∫¢ topics t·ª´ cluster '{cluster_name}'...")
        topics = client.get_topics(cluster_name)
        
        if not topics:
            logger.warning(f"‚ö†Ô∏è  Cluster '{cluster_name}' kh√¥ng c√≥ topic n√†o.")
            return
        
        logger.info(f"‚úÖ L·∫•y ƒë∆∞·ª£c {len(topics)} topics t·ª´ cluster '{cluster_name}'.")
        
        safe_cluster_name = sanitize_name_for_filename(cluster_name)
        successful_exports = 0
        failed_exports = 0
        empty_topics = 0
        
        # X·ª≠ l√Ω t·ª´ng topic
        for idx, topic in enumerate(topics, 1):
            topic_name = topic.get('name')
            if not topic_name:
                logger.warning(f"  [{idx}/{len(topics)}] Topic kh√¥ng c√≥ t√™n, b·ªè qua.")
                continue
            
            try:
                logger.info(f"  [{idx}/{len(topics)}] ƒêang l·∫•y {message_limit} message m·ªõi nh·∫•t t·ª´ topic '{topic_name}'...")
                
                # L·∫•y messages m·ªõi nh·∫•t
                messages = get_latest_messages_from_topic(
                    client=client,
                    topic_name=topic_name,
                    cluster_name=cluster_name,
                    limit=message_limit
                )
                
                if not messages:
                    logger.warning(f"    ‚ö†Ô∏è  Topic '{topic_name}' kh√¥ng c√≥ message n√†o.")
                    empty_topics += 1
                    # V·∫´n t·∫°o file JSON v·ªõi array r·ªóng
                    safe_topic_name = sanitize_name_for_filename(topic_name)
                    file_name = f"{safe_cluster_name}_{safe_topic_name}.json"
                    file_path = os.path.join(export_dir, file_name)
                    export_messages_to_json([], file_path)
                    continue
                
                # T·∫°o t√™n file: <Cluster_name>_<Topic_name>.json
                safe_topic_name = sanitize_name_for_filename(topic_name)
                file_name = f"{safe_cluster_name}_{safe_topic_name}.json"
                file_path = os.path.join(export_dir, file_name)
                
                # L∆∞u messages ra file JSON
                export_messages_to_json(messages, file_path)
                successful_exports += 1
                
            except Exception as e:
                logger.error(f"  ‚ùå L·ªói khi x·ª≠ l√Ω topic '{topic_name}': {e}")
                logger.debug(f"  Traceback: {traceback.format_exc()}")
                failed_exports += 1
                continue
        
        # T√≥m t·∫Øt k·∫øt qu·∫£
        logger.info("")
        logger.info("=" * 60)
        logger.info(f"üìä T√ìM T·∫ÆT K·∫æT QU·∫¢:")
        logger.info(f"   ‚úÖ Export th√†nh c√¥ng: {successful_exports} topic(s)")
        logger.info(f"   ‚ö†Ô∏è  Topic kh√¥ng c√≥ message: {empty_topics} topic(s)")
        logger.info(f"   ‚ùå L·ªói: {failed_exports} topic(s)")
        logger.info(f"   üìÅ Th∆∞ m·ª•c export: {export_dir}")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi extract messages t·ª´ cluster '{cluster_name}'", exc_info=True)
        logger.error(f"Traceback:\n{traceback.format_exc()}")


def main():
    """Main: r√† so√°t topics v√† extract messages ra JSON files."""
    parser = argparse.ArgumentParser(
        description="R√† so√°t t·∫•t c·∫£ topics trong cluster v√† l·∫•y 20 message m·ªõi nh·∫•t t·ª´ m·ªói topic, l∆∞u v√†o export-msgs"
    )
    parser.add_argument(
        "--cluster",
        type=str,
        default=None,
        help="T√™n cluster c·∫ßn x·ª≠ l√Ω (n·∫øu kh√¥ng ch·ªâ ƒë·ªãnh s·∫Ω d√πng cluster_name t·ª´ config.cfg)"
    )
    parser.add_argument(
        "--all-clusters",
        action="store_true",
        help="X·ª≠ l√Ω T·∫§T C·∫¢ clusters"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=20,
        help="S·ªë l∆∞·ª£ng messages t·ªëi ƒëa cho m·ªói topic (m·∫∑c ƒë·ªãnh: 20)"
    )
    
    args = parser.parse_args()
    
    # C·∫•u h√¨nh ƒë·ªçc t·ª´ config.cfg / bi·∫øn m√¥i tr∆∞·ªùng / tham s·ªë trong KafkaUIConfig
    config = KafkaUIConfig()
    client = KafkaUIClient(config)
    
    export_dir = ensure_export_msgs_dir()
    
    try:
        if args.all_clusters:
            # X·ª≠ l√Ω t·∫•t c·∫£ clusters
            logger.info("üîÑ ƒêang l·∫•y danh s√°ch T·∫§T C·∫¢ clusters...")
            clusters = client.get_clusters()
            logger.info(f"T√¨m th·∫•y {len(clusters)} clusters.")
            
            for cluster in clusters:
                cluster_name = cluster.get('name')
                if cluster_name:
                    logger.info("")
                    logger.info("=" * 60)
                    logger.info(f"üîÑ ƒêang x·ª≠ l√Ω cluster: '{cluster_name}'")
                    logger.info("=" * 60)
                    extract_messages_for_cluster(client, cluster_name, export_dir, args.limit)
        else:
            # X·ª≠ l√Ω cluster ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
            cluster_name = args.cluster or config.cluster_name
            if not cluster_name:
                logger.error("‚ùå Kh√¥ng c√≥ cluster n√†o ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh!")
                logger.info("üí° S·ª≠ d·ª•ng: --cluster <t√™n_cluster> ho·∫∑c --all-clusters")
                return
            
            extract_messages_for_cluster(client, cluster_name, export_dir, args.limit)
            
    except Exception as e:
        logger.error(f"‚ùå L·ªói ch√≠nh: {e}", exc_info=True)
        logger.error(f"Traceback ƒë·∫ßy ƒë·ªß:\n{traceback.format_exc()}")
        logger.info("\nüí° G·ª£i √Ω:")
        logger.info("  1. ƒê·∫£m b·∫£o Kafka UI ƒëang ch·∫°y v√† c·∫•u h√¨nh ƒë√∫ng trong config.cfg")
        logger.info("  2. Ki·ªÉm tra SESSION cookie ho·∫∑c username/password c√≥ ƒë√∫ng kh√¥ng")
        logger.info("  3. Ki·ªÉm tra t√™n cluster c√≥ ƒë√∫ng kh√¥ng")
        logger.info("  4. Th·ª≠: python main_extract_msg_from_topics.py --all-clusters")
        sys.exit(1)


if __name__ == "__main__":
    main()

